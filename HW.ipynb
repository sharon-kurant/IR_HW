{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sharonku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re, csv, string, pandas as pd\n",
    "from collections import defaultdict\n",
    "from numpy import loadtxt\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
    "fdist = FreqDist()\n",
    "\n",
    "dictionary = set()\n",
    "papers = {}\n",
    "path = \"HW1_TXT_files\"\n",
    "stop_words = []\n",
    "all_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of words in corpus is  536960\n",
      "Initial dictionary size is  26089\n"
     ]
    }
   ],
   "source": [
    "for file_name in os.listdir(path):\n",
    "    with open(path + os.sep + file_name, 'r', encoding = \"utf8\") as f:\n",
    "        papers[file_name.lower()] = tokenizer.tokenize(f.read())\n",
    "        for word in papers[file_name.lower()]:\n",
    "            fdist[word] += 1\n",
    "            all_words.append(word)\n",
    "        dictionary = dictionary.union(set(papers[file_name.lower()]))\n",
    "\n",
    "amount = sum(fdist.values())\n",
    "print(\"Amount of words in corpus is \", amount)    \n",
    "print(\"Initial dictionary size is \", len(dictionary))\n",
    "\n",
    "# generate language model\n",
    "for key, value in fdist.items():\n",
    "    fdist[key] = value / amount\n",
    "\n",
    "with open(\"stop_words_english.txt\", \"r\", encoding = \"utf8\") as stop_words_file:\n",
    "    stop_words = stop_words_file.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_stopwords = set()\n",
    "dictionary_casefold = set()\n",
    "dictionary_stemming = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words removal\n",
    "stopwords_filtered_list = [w for w in all_words if not w.lower() in stop_words]\n",
    "dictionary_stopwords = set(stopwords_filtered_list)\n",
    "# generate lang model\n",
    "\n",
    "fdist_stopwords = FreqDist(stopwords_filtered_list)\n",
    "stopwords_amount = sum(fdist_stopwords.values())\n",
    "for key, value in fdist_stopwords.items():\n",
    "    fdist_stopwords[key] = value / stopwords_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case folding\n",
    "casefold = [w.lower() for w in stopwords_filtered_list]\n",
    "dictionary_casefold = set(casefold)\n",
    "\n",
    "#generate lang model\n",
    "fdist_casefold = FreqDist(casefold)\n",
    "casefold_amount = sum(fdist_casefold.values())\n",
    "for key, value in fdist_casefold.items():\n",
    "    fdist_casefold[key] = value / casefold_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemming = [stemmer.stem(w) for w in casefold]\n",
    "dictionary_stemming = set(stemming)\n",
    "\n",
    "# generate stemmed lang model\n",
    "fdist_stemming = FreqDist(stemming)\n",
    "stemming_amount = sum(fdist_stemming.values())\n",
    "for key, value in fdist_stemming.items():\n",
    "    fdist_stemming[key] = value / stemming_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, paper in papers.items():\n",
    "    stopwords_filtered_list = [w for w in paper if not w.lower() in stop_words]\n",
    "    casefold = [w.lower() for w in stopwords_filtered_list]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemming = [stemmer.stem(w) for w in casefold]\n",
    "    papers[title] = stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial Dictionary size is  26089\n",
      "Dictionary size after stop words removal is  24872\n",
      "Dictionary size after case folding is  20064\n",
      "Dictionary size after stemming is  14363\n"
     ]
    }
   ],
   "source": [
    "print(\"initial Dictionary size is \", len(dictionary))\n",
    "\n",
    "print(\"Dictionary size after stop words removal is \", len(dictionary_stopwords))\n",
    "\n",
    "print(\"Dictionary size after case folding is \", len(dictionary_casefold))\n",
    "\n",
    "print(\"Dictionary size after stemming is \", len(dictionary_stemming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial language model\n",
      "FreqDist({'the': 0.062270932657926104, 'of': 0.03338609952324195, 'and': 0.028834550059594757, 'to': 0.021897348033373062, 'in': 0.018723927294398094, 'a': 0.017740613825983312, 'is': 0.009320992252681765, 'for': 0.009266984505363528, 'that': 0.009108685935637665, 'The': 0.007769666269368296, ...})\n",
      "\n",
      "language model after stop words removal\n",
      "FreqDist({'museum': 0.010523861221119589, 'mobile': 0.00907506230982125, 'guide': 0.008341106133121301, 'user': 0.008077340632119756, 'visitors': 0.007675958347986973, 'visitor': 0.004690438691723115, 'al': 0.003990886710805976, 'Museum': 0.003922078319240355, 'time': 0.003902964877138794, 'guides': 0.0037194758329638067, ...})\n",
      "\n",
      "language model after case folding\n",
      "FreqDist({'museum': 0.014656187403477117, 'mobile': 0.01151776021040077, 'guide': 0.009690515145491522, 'user': 0.009575834492882154, 'visitors': 0.008417559901527547, 'visitor': 0.005114757106377773, 'guides': 0.004384623618098136, 'context': 0.0042814110307497055, 'al': 0.004021468218168473, 'time': 0.004017645529748161, ...})\n",
      "\n",
      "language model after stemming\n",
      "FreqDist({'museum': 0.01736265080505818, 'guid': 0.014656187403477117, 'visitor': 0.013532317007905319, 'user': 0.013104175904830349, 'mobil': 0.011643908928271074, 'exhibit': 0.009828131928622762, 'visit': 0.005993975443049588, 'applic': 0.005837245217816786, 'interact': 0.005730209942048044, 'studi': 0.004862459670637166, ...})\n"
     ]
    }
   ],
   "source": [
    "print(\"initial language model\")\n",
    "fdist.pprint()\n",
    "\n",
    "print(\"\\nlanguage model after stop words removal\")\n",
    "fdist_stopwords.pprint()\n",
    "\n",
    "print(\"\\nlanguage model after case folding\")\n",
    "fdist_casefold.pprint()\n",
    "\n",
    "print(\"\\nlanguage model after stemming\")\n",
    "fdist_stemming.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
    "\n",
    "rand_papers = {}\n",
    "path = \"HW1_rand_txt\"\n",
    "\n",
    "for file_name in os.listdir(path):\n",
    "    with open(path + os.sep + file_name, 'r', encoding = \"utf8\") as f:\n",
    "        rand_papers[file_name.lower()] = tokenizer.tokenize(f.read())\n",
    "        \n",
    "\n",
    "for title, paper in rand_papers.items():\n",
    "    stopwords_filtered_list = [w for w in paper if not w.lower() in stop_words]\n",
    "    casefold = [w.lower() for w in stopwords_filtered_list]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemming = [stemmer.stem(w) for w in casefold]\n",
    "    rand_papers[title] = FreqDist(stemming)\n",
    "    words_amount = sum(rand_papers[title].values())\n",
    "    \n",
    "    for key, value in rand_papers[title].items():\n",
    "        rand_papers[title][key] = value / words_amount\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_list = ['A Framework for Guiding the Museum Tours Personalization',\n",
    "            'A Multi-Sensory Approach to Cultural Heritage The Battle of Pavia Exhibition',\n",
    "            'A Novel Image Based Positioning Technique Using Mobile Eye Tracker For A Museum Visit',\n",
    "            'A Point-Of-Interest Directory For Mobile Tourists In Abuja, Nigeria',\n",
    "            'A Survey of Map-based Mobile Guides',\n",
    "            'A visitors guide in an active museum Presentation',\n",
    "            'Adoption and Use of Emerging Cultural Technologies in China\\'s Museums',\n",
    "            'Analyzing Visitor Perceptions of Personalization in Art Museum Interactive Technology',\n",
    "            'Augmented reality for visitors of cultural heritage sites',\n",
    "            'Design and development of Taeneb City Guide - From Paper Maps and Guidebooks to Electronic Guides',\n",
    "            'Full Access to Cultural Spaces (FACS) Mapping and evaluating museum access services using mobile eye-tracking technology',\n",
    "            'In-Sights into Mobile Learning  An Exploration of Mobile Eye Tracking Methodology for  Learning in Museums',\n",
    "            'Mobile augmented reality for cultural heritage Following the footsteps of Ovid among different locations in Europe',\n",
    "            'Museum Guide 2.0 – An Eye-Tracking based Personal Assistant for Museums and Exhibits',\n",
    "            'Potentials and Limitations of Mobile Eye Tracking in Visitor Studies_ Evidence From Field Research at Two Museum Exhibitions in Germany',\n",
    "            ]\n",
    "\n",
    "rel_list = [name.lower() + '.txt' for name in rel_list]\n",
    "len(rel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
    "\n",
    "rand_papers_clas = {}\n",
    "path = \"HW1_rand_txt\"\n",
    "\n",
    "for file_name in os.listdir(path):\n",
    "    with open(path + os.sep + file_name, 'r', encoding = \"utf8\") as f:\n",
    "        if file_name.lower() in rel_list:\n",
    "            rand_papers_clas[file_name.lower()] = (f.read(), 1)\n",
    "        else:\n",
    "            rand_papers_clas[file_name.lower()] = (f.read(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2da4d3d9bcc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrand_papers_clas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_train_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(rand_papers_clas.values())\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012224374255065555"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for initial language model\n",
    "fdist[\"mobile\"]+fdist[\"visitors\"]+fdist[\"guide\"]\n",
    "\n",
    "#for language model after stop words removal\n",
    "\n",
    "# for language model after case folding\n",
    "\n",
    "# for language model after stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Museum Guide 2.0 – An Eye-Tracking based Personal Assistant for Museums and Exhibits.txt',\n",
       " 'Potentials and Limitations of Mobile Eye Tracking in Visitor Studies_ Evidence From Field Research at Two Museum Exhibitions in Germany.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_test = rel_list[-2:]\n",
    "rel_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
